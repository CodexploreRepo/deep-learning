# Batch Normalization
- Batch Norm is a neural network layer that is now commonly used in many architectures. 
- It often gets added as part of a Linear or Convolutional block and helps to stabilize the network during training.
- Batch Normalization was recognized as being transformational in creating deeper neural networks that could be trained faster.
